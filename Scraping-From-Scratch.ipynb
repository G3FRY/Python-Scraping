{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the url and getting the response\n",
    "url = 'https://boston.craigslist.org/search/jjj?'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the content of the response if status code is 200\n",
    "if response.status_code == 200:\n",
    "    data = response.text\n",
    "else:\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Soup object and find all 'a' tags\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "tags = soup.find_all('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a dictionary to specify element details\n",
    "results = soup.find_all('a', {'class': 'result-title'})\n",
    "for result in results:\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the address information\n",
    "addresses = soup.find_all('span', {'class':'result-hood'})\n",
    "for address in addresses:\n",
    "    print(address.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the address and the titles belong to a div tag parent, we call it \"wrapper\"\n",
    "details = soup.find_all('div', {'class':'result-info'})\n",
    "\n",
    "# creating dataframe to export results\n",
    "column_names = ['name','city','date','link','description','compensation','employment_type','posting_info']\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for detail in details:\n",
    "    job_info = []\n",
    "    name = detail.find('a', {'class': 'result-title'}).text\n",
    "    # in case a location doesn't appear we can add an if statement\n",
    "    city_tag = detail.find('span', {'class':'result-hood'})\n",
    "    city = city_tag.text[2:-1] if city_tag else \"N/A\"\n",
    "    date = detail.find('time', {'class':'result-date'}).text\n",
    "    link = detail.find('a', {'class':'result-title'}).get('href')\n",
    "    \n",
    "    # getting some detail info about the jobs\n",
    "    try:\n",
    "        job_detail_response = requests.get(link)\n",
    "        job_soup = BeautifulSoup(job_detail_response.text, 'html.parser')\n",
    "        # getting section of interest\n",
    "        job_detail = job_soup.find('section', {'class':'userbody'})\n",
    "        description = job_detail.find('section', {'id':'postingbody'}).text\n",
    "        compensation_type_tag = job_detail.find('p', {'class':'attrgroup'})\n",
    "        if compensation_type_tag:\n",
    "            span_tags = compensation_type_tag.find_all('span')\n",
    "            for span_tag in span_tags:\n",
    "                if 'compensation' in span_tag.text.lower():\n",
    "                    compensation = span_tag.text.split(':')[1].strip()\n",
    "                elif 'employment type' in span_tag.text.lower():\n",
    "                    employment_type = span_tag.text.split(':')[1].strip()\n",
    "        else:\n",
    "            compensation = ''\n",
    "            employment_type = ''\n",
    "\n",
    "        posting_info = job_detail.find_all('p', {'class':'postinginfo'})[1].text.split(':')[1].strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error getting job details!\")\n",
    "        print(e)\n",
    "        description = ''\n",
    "        compensation = ''\n",
    "        employment_type = ''\n",
    "        posting_info = ''\n",
    "\n",
    "    job_info.append(name)\n",
    "    job_info.append(city)\n",
    "    job_info.append(date)\n",
    "    job_info.append(link)\n",
    "    job_info.append(description)\n",
    "    job_info.append(compensation)\n",
    "    job_info.append(employment_type)\n",
    "    job_info.append(posting_info)\n",
    "\n",
    "    df.loc[len(df.index)] = job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export result to current folder\n",
    "df.to_csv(r'.\\\\Results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c4db929531b659fdf648c720fbbaaf9e55e1b9805d10aedc2b762f89386134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
