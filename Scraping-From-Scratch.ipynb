{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the url and getting the response\n",
    "url = 'https://boston.craigslist.org/search/jjj?'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the content of the response if status code is 200\n",
    "if response.status_code == 200:\n",
    "    data = response.text\n",
    "else:\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Soup object and find all 'a' tags\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "tags = soup.find_all('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a dictionary to specify element details\n",
    "results = soup.find_all('a', {'class': 'result-title'})\n",
    "for result in results:\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the address information\n",
    "addresses = soup.find_all('span', {'class':'result-hood'})\n",
    "for address in addresses:\n",
    "    print(address.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the address and the titles belong to a div tag parent, we call it \"wrapper\"\n",
    "details = soup.find_all('div', {'class':'result-info'})\n",
    "\n",
    "# creating dataframe to export results\n",
    "column_names = ['name','city','date','link','description','compensation','employment_type','posting_info']\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for detail in details:\n",
    "    job_info = []\n",
    "    name = detail.find('a', {'class': 'result-title'}).text\n",
    "    # in case a location doesn't appear we can add an if statement\n",
    "    city_tag = detail.find('span', {'class':'result-hood'})\n",
    "    city = city_tag.text[2:-1] if city_tag else \"N/A\"\n",
    "    date = detail.find('time', {'class':'result-date'}).text\n",
    "    link = detail.find('a', {'class':'result-title'}).get('href')\n",
    "    \n",
    "    # getting some detail info about the jobs\n",
    "    try:\n",
    "        job_detail_response = requests.get(link)\n",
    "        job_soup = BeautifulSoup(job_detail_response.text, 'html.parser')\n",
    "        # getting section of interest\n",
    "        job_detail = job_soup.find('section', {'class':'userbody'})\n",
    "        description = job_detail.find('section', {'id':'postingbody'}).text\n",
    "        compensation_type_tag = job_detail.find('p', {'class':'attrgroup'})\n",
    "        if compensation_type_tag:\n",
    "            span_tags = compensation_type_tag.find_all('span')\n",
    "            for span_tag in span_tags:\n",
    "                if 'compensation' in span_tag.text.lower():\n",
    "                    compensation = span_tag.text.split(':')[1].strip()\n",
    "                elif 'employment type' in span_tag.text.lower():\n",
    "                    employment_type = span_tag.text.split(':')[1].strip()\n",
    "        else:\n",
    "            compensation = ''\n",
    "            employment_type = ''\n",
    "\n",
    "        posting_info = job_detail.find_all('p', {'class':'postinginfo'})[1].text.split(':')[1].strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error getting job details!\")\n",
    "        print(e)\n",
    "        description = ''\n",
    "        compensation = ''\n",
    "        employment_type = ''\n",
    "        posting_info = ''\n",
    "\n",
    "    job_info.append(name)\n",
    "    job_info.append(city)\n",
    "    job_info.append(date)\n",
    "    job_info.append(link)\n",
    "    job_info.append(description)\n",
    "    job_info.append(compensation)\n",
    "    job_info.append(employment_type)\n",
    "    job_info.append(posting_info)\n",
    "\n",
    "    df.loc[len(df.index)] = job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export result to current folder\n",
    "df.to_csv(r'.\\\\Results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what you have leaned in this course, web scrape the API lists on this page, and export your result into a CSV file.\n",
    "\n",
    "https://www.programmableweb.com/category/all/apis\n",
    "\n",
    "OR\n",
    "\n",
    "https://www.programmableweb.com/category/tools/api\n",
    "\n",
    "\n",
    "\n",
    "Your Python code should scrape the following details from each table row:\n",
    "\n",
    "• API Name  \n",
    "\n",
    "• API (absolute) URL  \n",
    "\n",
    "• API Category  \n",
    "\n",
    "• API Description  \n",
    "\n",
    "\n",
    "\n",
    "Your Python code should crawl to all the available \"next\" pages. Your final result should be approx. 20,400 rows, and for the second link approx. 1533 rows.\n",
    "\n",
    "\n",
    "\n",
    "On your machine, write the code and test it to make sure it is working. Then copy paste your final code here.\n",
    "\n",
    "\n",
    "\n",
    "All the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL\n",
    "url = 'https://www.programmableweb.com'\n",
    "\n",
    "# define URL\n",
    "base_url = 'https://www.programmableweb.com/category/tools/api?pw_view_display_id=apis_all&page='\n",
    "\n",
    "# counter to concatenate with url\n",
    "count_url = 0\n",
    "\n",
    "# counter to keep track of each result\n",
    "count = 0\n",
    "\n",
    "# dictionary where results will be saved\n",
    "results = {}\n",
    "column_names = ['Name', 'Absolute URL', 'Category', 'Description']\n",
    "\n",
    "while True:\n",
    "    specific_url = 'https://www.programmableweb.com/category/tools/api?pw_view_display_id=apis_all&page=' + str(count_url)\n",
    "    print('Getting data from ' + specific_url)\n",
    "    response = requests.get(specific_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class':'views-table'})\n",
    "    if table is None:\n",
    "        print(\"No more results!\")\n",
    "        break   \n",
    "    body = table.find('tbody')\n",
    "    rows = body.find_all('tr')\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "        name = data[0].text if data[0] else \"N/A\"\n",
    "        partial_api_url = data[4].find('a').get('href')\n",
    "        # a href equals to # means a dropdown is present\n",
    "        if partial_api_url == \"#\":\n",
    "            # when the version field has a dropdown we check for li tags\n",
    "            versions = data[4].find_all('li')\n",
    "            for version in versions:\n",
    "                # checking and selecting the last recommended version\n",
    "                if 'recommended' in version.text.lower():\n",
    "                    api_url = url + version.find('a').get('href')     \n",
    "        else:\n",
    "            api_url = url + partial_api_url\n",
    "        category = data[2].text if data[2] else \"N/A\" \n",
    "        detail_response = requests.get(api_url)\n",
    "        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
    "        detail = detail_soup.find('div', {'class':'api_description'}).text\n",
    "        results[count] = [name, api_url, category, detail]\n",
    "        count += 1\n",
    "    count_url += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe from result dictionary\n",
    "df = pd.DataFrame.from_dict(results, orient='index', columns=column_names)\n",
    "df.to_csv('./API_Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c4db929531b659fdf648c720fbbaaf9e55e1b9805d10aedc2b762f89386134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
